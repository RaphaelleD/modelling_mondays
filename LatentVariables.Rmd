---
title: "Introduction to Latent Variables"
author: "Miranda Copps"
date: "`r Sys.Date()`"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lavaan)
library(semPlot)

```

# Latent Variable Models

Today we're going to discuss the use of latent variable models. These fall under the category of structural equation models (SEM). SEM are a broad and flexible family of methods which can be used to model complex relationships between variables. There can be multiple dependent variables and a mix of observed and latent variables.

In psychology, we often cannot directly observe what we wish to measure e.g. intelligence, personality traits, depression. So instead we use observable indicators to infer these characteristics e.g. scores on tests.

Something which might be confusing to get out of the way is that the same "thing" could be treated as an observed variable in one context and a latent variable in another. E.g. in questionnaire development we might want to develop a latent model for e.g. depression. Once this questionnaire is developed we may then take the observed depression score on that questionnaire to develop a latent variable model of "psychopathology", along with other questionnaires. It depends on what you're trying to do.

## A Simple Example

Let's start with an illustrative example, measuring temperature, which I have borrowed from [@speekenbrink]. We cannot directly see temperature, so we rely on instruments (thermometers) to give us measurements. Briefly, an increase in temperature (i.e. an increase in kinetic energy) *causes* the volume of mercury to expand which pushes it further up the tube of the thermometer. We can then take a reading from the lines on the thermometer to tell us (an approximation of) the current temperature.

We can represent this graphically.

```{r temp}

tem <- data.frame(x = rnorm(100)) %>%
  dplyr::mutate(measure = 30 + 10*x)
tem_spec <- '
temperature =~ measure
'

tem_mod <- lavaan::cfa(tem_spec, data=tem)
semPaths(tem_mod, sizeMan=7, sizeInt = 4, style="mx", residuals=TRUE, normalize=FALSE, width=2, height=2, rotation=2, nCharNodes = 0, fixedStyle = 1)
```

Diagrams in SEM have specific conventions which tell us about the model:

-   Squares represent *observed* variables
-   Circles represent *latent* variables
-   Triangles represent (known) *constants* (intercepts or means)
-   One-directional (single-headed) arrows represent a *causal relation* between two variables
-   Bi-directional (double-headed) arrows represent *non-causal relations* (e.g. covariance or correlation). A bi-directional arrow from one variable onto itself represents the (residual) variance of a variable (which technically is equal to the covariance of a variable with itself)
-   Broken arrows represent *fixed* parameters. These parameters are assumed to be equal to the true parameter values, rather than estimates of the true values.

This single item (i.e. a single measurement), can also be represented with a linear model:

$$
measurement_1 = \tau_1+ \lambda_1 \times temperature_1 + \epsilon_1
$$

Or more generally this can be written as $$
y_1 = \tau_1+ \lambda_1 \times \eta_1 + \epsilon_1
$$

$\tau$ intercept of the first item (also commonly represented as $\alpha$)

$\lambda$ is the loading or regression weight of the first factor on the first item

$\eta$ is the unobserved variable (unlike in a typical regression where $x$ is observed). We assume this is normally distributed, in order to infer its properties from $y$)

$\epsilon$ is the error, also normally distributed

So we have a linear regression, where the main predictor is unobserved, this is the fundamental model of CFA. Remember that in a factor model, our outcome is an item not an observation.

## Confirmatory Factor Models (CFA)

In the example above we have one observed variable (measure from the thermometer) and one latent variable (temperature). In practice, we will have multiple indicators (e.g. multiple test scores or multiple items on a questionnaire) for each latent variable. In fact, having at least three indicators is a requirement of an adequately *identified* CFA model. (This means that our degrees of freedom for the model are 0 or greater, I won't demonstrate the details of this now but I am mentioning it as it's an important criterion of CFA). It is through the relationships between these observed variables, that we can estimate our model parameters.

With multiple observed variables (but still a single latent variable), our equation from above becomes a matrix equation.

$$
\begin{pmatrix} y_1 \\ y_2 \\ y_3 \end{pmatrix} = \begin{pmatrix} \tau_1 \\ \tau_2 \\ \tau_3 \end{pmatrix} +  \begin{pmatrix} \lambda_1 \\ \lambda_2 \\ \lambda_3 \end{pmatrix} \eta_1 + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \end{pmatrix}
$$

$\tau$ is the item intercepts or means (we can often ignore these by standardising data)

$\lambda$ is the loadings, which can be interpreted as the correlation of the item with the factor $\eta$ is the latent predictor of the items

$\epsilon$ is the residuals of the factor model, whatâ€™s left over after accounting for the factor

From now on, let's use an example which is more relevant to psychology, intelligence. This is also historically relevant to the topic of CFA as a seminal use of factor analyses was Spearman's modelling of intelligence [@spearman1904]. Our matrix equation can be applied to intelligence as follows:

$$
\begin{pmatrix} testscore_1 \\ testscore_2 \\ testscore_3 \end{pmatrix} = \begin{pmatrix} \tau_1 \\ \tau_2 \\ \tau_3 \end{pmatrix} +  \begin{pmatrix} \lambda_1 \\ \lambda_2 \\ \lambda_3 \end{pmatrix} intelligence_1 + \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \end{pmatrix}
$$

## The (Observed and Model Implied) Variance Covariance Matrix

So far we have just looked at having one subject. Of course, in practice this isn't interesting. Rather, we want to know how much variance is common across a set of items within a population. To do this we need to consider the variance-covariance matrix.

From the data, we can calculate the observed variance-covariance matrix $\mathbf{\Sigma = Y'Y}$. If we z-transform our data to have a mean of zero and sd of 1 then this will equal the correlation matrix. It contains the variances $\sigma_j^2$ of all variables in the model, and the covariances of all pairs of variables $\sigma_{i,j}$

$$
\Sigma = \begin{pmatrix} \sigma^2_1 & \sigma_{1,2} & \sigma_{1,3}  \\ \sigma_{2,1} & \sigma^2_2 & \sigma_{2,3} \\ \sigma_{3,1} & \sigma_{3,2} & \sigma^2_3  \end{pmatrix}
$$

***Question for the group: Do we want to pause here and remind ourselves of some basics of matrices?***

The CFA model gives rise to a variance covariance matrix. Intuituvely, this is called the **model implied variance covariance matrix** $\Sigma(\Theta)$. Which can be expressed in

$$
\mathbf{\Sigma(\Theta) = \Lambda \Psi \Lambda` + \Theta_\epsilon}
$$

Which corresponds to the parameters estimated in the model: $\Lambda$ is the factor loading matrix (the same $\lambda$s as above) $\Psi$ is the variance-covariance matrix of the latent factors (with just one factor it is just the variance of the factor $\eta$) and $\Theta_\epsilon$ is the variance-covariance matrix of the residuals.

Essentially the CFA tries to find parameters which give rise to a model implied variance covariance matrix which fits the observed variance covariance matrix as closely as possible.

The dimensions of the $\mathbf{\Sigma}$ and $\mathbf{\Sigma(\Theta)}$ are the same, in this case $3 \times 3$. If we define the right hand side of the matrix equation, for a one-factor three-item case above we have:

$$
\Sigma(\Theta) = \begin{pmatrix} \lambda_1 \\ \lambda_2 \\ \lambda_3 \end{pmatrix} (\psi_{11}) (\lambda_1 \lambda_2 \lambda_3) + \begin{pmatrix} \theta_{11} & \theta_{12} & \theta_{13}  \\ \theta_{21} & \theta_{22} & \theta_{23}  \\ \theta_{31} & \theta_{32} & \theta_{33}  \end{pmatrix}
$$

As $\lambda$s are the same parameters in the measurement model (i.e. the simple linear regression from the first section), the only new parameters are the $\Psi$ and $\Theta_\epsilon$.

So far, this is quite abstract so let's go back to our example of intelligence. To illustrate CFA we will use some example data, from the Lavaan package. The data comprises various mental ability test scores for 301 children.

```{r load}
# We'll just select the nine variables with test scores, ignoring demographics etc (vars x1:x9)
dat <- HolzingerSwineford1939 %>%
  select(num_range("x", 1:9, suffix = ""))

```

There are nine scores in total, but to start lets look at just three variables with are related to the participants processing speed (variables 7, 8 and 9). We want to view the correlation matrix for these variables to see if they are related, and therefore suitable for factor analysis.

```{r cor}
# For ease later on, we will z-transform the data (this allows us to ignore the means and is also important when the observed variables don't have the same scales as each other)
# Due to scaling, our cor matrix == the observed variance-covariance matrix
dat <- as.data.frame(scale(dat)) 

# cor
round(cor(dat[7:9]), 3)
```

The correlations range from 0.34 to 0.49 which is good (a rule of thumb is that the observed correlations between items should 0.3 to 0.8 for factor analyses).

Before we run a CFA on this data we need to add two constraints to our model. These allow the model to be just just identified (again, we won't go into all the details as we know the basic principle that df must be $\ge0$).

1.  We assume the residuals of each variable are independent, so in the model implied covariance matrix these elements are fixed to 0
2.  We fix the first loadings of each factor to 1

$$
\Sigma(\Theta) = \begin{pmatrix} 1 \\ \lambda_2 \\ \lambda_3 \end{pmatrix} (\psi_{11}) (1 \lambda_2 \lambda_3) + \begin{pmatrix} \theta_{11} & 0 & 0  \\ 0 & \theta_{22} & 0  \\ 0 & 0 & \theta_{33}  \end{pmatrix}
$$

Now we can run the one-factor, three-item CFA and look for the model parameters in the output (looking at the Std.lv values in particular, which means that all estimates are standardized by the predictor).

```{r cfa}
# define the model
cfa_spec <- '
spe =~ x7 + x8 + x9'

# fit the model
fit <- cfa(model = cfa_spec, data = dat)
summary(fit, fit.measures = TRUE, standardized = TRUE)
```

Let's also plot this model.

```{r Plot}

semPaths(fit, sizeMan=7, sizeInt = 4, what = "std", style = "ram", residuals=TRUE, normalize=FALSE, width=2, height=2, rotation=2, nCharNodes = 0, fixedStyle = 1, edge.label.cex = 1)

```

Finally, we can see the model implied variance covariance matrix. Because our model is just-identified this is identical (plus some rounding errors) to the observed variance covariance matrix.

```{r VarCovMod}

inspect(fit,"cov.ov")

```

## Quick Exercise

Can you find all the terms of the matrix equation in the Lavaan model output?

**Today I haven't covered the following important topics (among others):**

1.  Model estimation using maximum likelihood or weighted least square. The former assumes multivariate normality whereas the latter does not but requires larger samples.
2.  Assessment of model fit, for which there are many possible methods (e.g. errors of approximation, comparative fit to a baseline, model comparison for non-nested models) . It is common to use these in combination to increase confidence in our judgement of fit.

## Key materials and references:

1.  A useful textbook on many stats topics, authored by a UCL professor "Statistics: Data analysis and modelling", Maarten Speekenbrink, 2023-11-26, <https://mspeekenbrink.github.io/sdam-book/index.html> <https://stats.oarc.ucla.edu/r/seminars/rcfa/#s1>
2.  UCLA's tutorial on CFA is another great resource, and matches the structure of the present talk more closely <https://stats.oarc.ucla.edu/r/seminars/rcfa/>
3.  The Lavaan tutorial on CFA <https://lavaan.ugent.be/tutorial/cfa.html>
